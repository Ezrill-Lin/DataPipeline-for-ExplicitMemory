{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, load_from_disk, concatenate_datasets\n",
    "from MinHashDeduplication import Deduplicate\n",
    "from RuleBasedFilter import RuleBasedFilter\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 395000\n",
      "Deduplicated dataset size: 50443\n",
      "Original dataset size: 50443\n",
      "Filtered dataset size: 50229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 50229/50229 [00:00<00:00, 1183790.79 examples/s]\n",
      " 20%|██        | 1/5 [05:14<20:59, 314.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset meta-math/MetaMathQA cleaning completed. \n",
      "\n",
      "Original dataset size: 16006\n",
      "Deduplicated dataset size: 15056\n",
      "Original dataset size: 15056\n",
      "Filtered dataset size: 11803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 11803/11803 [00:00<00:00, 383215.96 examples/s]\n",
      " 40%|████      | 2/5 [05:44<07:20, 146.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset LDJnr/Capybara cleaning completed. \n",
      "\n",
      "Original dataset size: 18612\n",
      "Deduplicated dataset size: 17520\n",
      "Original dataset size: 17520\n",
      "Filtered dataset size: 17468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 17468/17468 [00:00<00:00, 881460.34 examples/s]\n",
      " 60%|██████    | 3/5 [06:04<02:58, 89.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset iamtarun/python_code_instructions_18k_alpaca cleaning completed. \n",
      "\n",
      "Original dataset size: 93733\n",
      "Deduplicated dataset size: 85158\n",
      "Original dataset size: 85158\n",
      "Filtered dataset size: 83834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (9/9 shards): 100%|██████████| 83834/83834 [00:08<00:00, 10154.19 examples/s]\n",
      " 80%|████████  | 4/5 [12:54<03:35, 215.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset open-r1/OpenR1-Math-220k cleaning completed. \n",
      "\n",
      "Original dataset size: 100000\n",
      "Deduplicated dataset size: 62670\n",
      "Original dataset size: 62670\n",
      "Filtered dataset size: 62659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 62659/62659 [00:00<00:00, 377993.43 examples/s]\n",
      "100%|██████████| 5/5 [23:42<00:00, 284.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset hkust-nlp/CodeIO-PyEdu-Reasoning cleaning completed. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = {\n",
    "        \"meta-math/MetaMathQA\": ['train', 'query', None, False],\n",
    "        \"LDJnr/Capybara\": ['train', 'conversation', 'conversation', False],\n",
    "        \"iamtarun/python_code_instructions_18k_alpaca\": ['train', 'instruction', None, False],\n",
    "        \"open-r1/OpenR1-Math-220k\": ['train', 'problem', None, False],\n",
    "        \"hkust-nlp/CodeIO-PyEdu-Reasoning\": ['train', 'prompt', None, True],\n",
    "    }\n",
    "\n",
    "def run_filter(raw_datasets):\n",
    "    for name, l in tqdm(raw_datasets.items()):\n",
    "        streaming = l[3]\n",
    "        if not streaming:\n",
    "            ds = load_dataset(name, split=l[0])\n",
    "        else:\n",
    "            ds = []\n",
    "            ds_stream = load_dataset(name, split=l[0], streaming=True)\n",
    "            for i, sample in enumerate(ds_stream):\n",
    "                if i >= 100000:\n",
    "                    break\n",
    "                ds.append(sample)\n",
    "        \n",
    "        dd_filter = Deduplicate(ds, l[1])\n",
    "        ds_dd = dd_filter.run()\n",
    "        rb_filter = RuleBasedFilter(ds_dd, l[2])\n",
    "        ds_rb = rb_filter.run()\n",
    "        ds_cleaned = Dataset.from_list(ds_rb)\n",
    "        ds_cleaned.save_to_disk(f'./cleaned_datasets/{name}_cleaned')\n",
    "        print(f'Dataset {name} cleaning completed. \\n')\n",
    "\n",
    "run_filter(raw_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 26.80it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 92351.58 examples/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 31.24it/s]\n"
     ]
    }
   ],
   "source": [
    "def dataset_concat(raw_datasets, samp=False):\n",
    "    ds_list = []\n",
    "    for name in tqdm(raw_datasets.keys()):\n",
    "        if not samp:\n",
    "            ds_list.append(load_from_disk(f'./cleaned_datasets/{name}_cleaned'))\n",
    "        else:\n",
    "            ds_list.append(load_from_disk(f'./cleaned_datasets/{name}_cleaned').select(range(2000)))\n",
    "\n",
    "    ds_concat = concatenate_datasets(ds_list)\n",
    "    ds_concat = ds_concat.add_column('index', range(len(ds_concat)))\n",
    "\n",
    "    return ds_concat\n",
    "\n",
    "samp_to_label = dataset_concat(raw_datasets, samp=True)\n",
    "samp_to_label.save_to_disk('./cleaned_datasets/sample_to_label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extract(sample):\n",
    "    if sample['query']:\n",
    "        text = sample['query'] + sample['response']\n",
    "        dataset = 'metamath'\n",
    "    elif sample['conversation']:\n",
    "        text = []\n",
    "        for t in sample['conversation']:\n",
    "            text.append(\" \".join(t.values()))\n",
    "        text = \" \".join(text)\n",
    "        dataset = 'capybara'\n",
    "    elif sample['instruction']:\n",
    "        text = sample['prompt']\n",
    "        dataset = 'code18k'\n",
    "    elif sample['problem']:\n",
    "        text = sample['problem'] + sample['solution'] + sample['answer']\n",
    "        dataset = 'openmath'\n",
    "    elif sample['turn_1']:\n",
    "        try:\n",
    "            text = sample['prompt'] + sample['turn_1'] + sample['feedback_1'] + sample['turn_2'] + sample['feedback_2']\n",
    "        except:\n",
    "            text = sample['prompt'] + sample['turn_1'] + sample['feedback_1']\n",
    "        dataset = 'codeio'\n",
    "    else:\n",
    "        text = None\n",
    "        dataset = None\n",
    "    return {'text': f'{text}',\n",
    "            'dataset': f'{dataset}'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 28.32it/s]\n",
      "Saving the dataset (11/11 shards): 100%|██████████| 225993/225993 [00:04<00:00, 50951.29 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'text', 'dataset'],\n",
       "    num_rows: 225993\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = dataset_concat(raw_datasets)\n",
    "all_data.save_to_disk('all_data')\n",
    "all_data_shuffled6 = all_data.shuffle(seed=6)\n",
    "\n",
    "texts = all_data_shuffled6.map(text_extract)\n",
    "texts_extracted = texts.remove_columns([col for col in texts.column_names \\\n",
    "                                        if col != 'text' and col != 'dataset' and col != 'index'])\n",
    "        \n",
    "texts_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225993/225993 [00:07<00:00, 28820.20it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('./jsonl/all_data_to_bert.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for text in tqdm(texts_extracted):\n",
    "        f.write(json.dumps(text) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
