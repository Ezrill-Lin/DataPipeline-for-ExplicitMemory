{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linxi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, load_from_disk, concatenate_datasets\n",
    "from MinHashDeduplication import Deduplicate\n",
    "from RuleBasedFilter import RuleBasedFilter\n",
    "from TinyBERTFilter import TinyBERTFilter\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = {\n",
    "        \"meta-math/MetaMathQA\": ['train', 'query', None, False],\n",
    "        \"LDJnr/Capybara\": ['train', 'conversation', 'conversation', False],\n",
    "        \"iamtarun/python_code_instructions_18k_alpaca\": ['train', 'instruction', None, False],\n",
    "        \"open-r1/OpenR1-Math-220k\": ['train', 'problem', None, False],\n",
    "        \"hkust-nlp/CodeIO-PyEdu-Reasoning\": ['train', 'prompt', None, True],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 395000\n",
      "Deduplicated dataset size: 50443\n",
      "Original dataset size: 50443\n",
      "Filtered dataset size: 50229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 50229/50229 [00:00<00:00, 1183790.79 examples/s]\n",
      " 20%|██        | 1/5 [05:14<20:59, 314.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset meta-math/MetaMathQA cleaning completed. \n",
      "\n",
      "Original dataset size: 16006\n",
      "Deduplicated dataset size: 15056\n",
      "Original dataset size: 15056\n",
      "Filtered dataset size: 11803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 11803/11803 [00:00<00:00, 383215.96 examples/s]\n",
      " 40%|████      | 2/5 [05:44<07:20, 146.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset LDJnr/Capybara cleaning completed. \n",
      "\n",
      "Original dataset size: 18612\n",
      "Deduplicated dataset size: 17520\n",
      "Original dataset size: 17520\n",
      "Filtered dataset size: 17468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 17468/17468 [00:00<00:00, 881460.34 examples/s]\n",
      " 60%|██████    | 3/5 [06:04<02:58, 89.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset iamtarun/python_code_instructions_18k_alpaca cleaning completed. \n",
      "\n",
      "Original dataset size: 93733\n",
      "Deduplicated dataset size: 85158\n",
      "Original dataset size: 85158\n",
      "Filtered dataset size: 83834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (9/9 shards): 100%|██████████| 83834/83834 [00:08<00:00, 10154.19 examples/s]\n",
      " 80%|████████  | 4/5 [12:54<03:35, 215.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset open-r1/OpenR1-Math-220k cleaning completed. \n",
      "\n",
      "Original dataset size: 100000\n",
      "Deduplicated dataset size: 62670\n",
      "Original dataset size: 62670\n",
      "Filtered dataset size: 62659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 62659/62659 [00:00<00:00, 377993.43 examples/s]\n",
      "100%|██████████| 5/5 [23:42<00:00, 284.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset hkust-nlp/CodeIO-PyEdu-Reasoning cleaning completed. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_filter(raw_datasets):\n",
    "    for name, l in tqdm(raw_datasets.items()):\n",
    "        streaming = l[3]\n",
    "        if not streaming:\n",
    "            ds = load_dataset(name, split=l[0])\n",
    "        else:\n",
    "            ds = []\n",
    "            ds_stream = load_dataset(name, split=l[0], streaming=True)\n",
    "            for i, sample in enumerate(ds_stream):\n",
    "                if i >= 100000:\n",
    "                    break\n",
    "                ds.append(sample)\n",
    "        \n",
    "        dd_filter = Deduplicate(ds, l[1])\n",
    "        ds_dd = dd_filter.run()\n",
    "        rb_filter = RuleBasedFilter(ds_dd, l[2])\n",
    "        ds_rb = rb_filter.run()\n",
    "        ds_cleaned = Dataset.from_list(ds_rb)\n",
    "        ds_cleaned.save_to_disk(f'./datasets/{name}_cleaned')\n",
    "        print(f'Dataset {name} cleaning completed. \\n')\n",
    "\n",
    "run_filter(raw_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatnating: 100%|██████████| 5/5 [00:00<00:00, 30.79it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 101271.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def dataset_concat(raw_datasets, samp=False):\n",
    "    ds_list = []\n",
    "    for name in tqdm(raw_datasets.keys(), desc='Concatnating'):\n",
    "        if not samp:\n",
    "            ds_list.append(load_from_disk(f'./datasets/{name}_cleaned'))\n",
    "        else:\n",
    "            ds_list.append(load_from_disk(f'./datasets/{name}_cleaned').select(range(2000)))\n",
    "\n",
    "    ds_concat = concatenate_datasets(ds_list)\n",
    "    ds_concat = ds_concat.add_column('index', range(len(ds_concat)))\n",
    "\n",
    "    return ds_concat\n",
    "\n",
    "samp_to_label = dataset_concat(raw_datasets, samp=True)\n",
    "samp_to_label.save_to_disk('./datasets/sample_to_label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_or_dataset(sample, add_text=True):\n",
    "    if sample['query']:\n",
    "        text = sample['query'] + sample['response']\n",
    "        dataset = 'metamath'\n",
    "    elif sample['conversation']:\n",
    "        text = []\n",
    "        for t in sample['conversation']:\n",
    "            text.append(\" \".join(t.values()))\n",
    "        text = \" \".join(text)\n",
    "        dataset = 'capybara'\n",
    "    elif sample['instruction']:\n",
    "        text = sample['prompt']\n",
    "        dataset = 'code18k'\n",
    "    elif sample['problem']:\n",
    "        text = sample['problem'] + sample['solution'] + sample['answer']\n",
    "        dataset = 'openmath'\n",
    "    elif sample['turn_1']:\n",
    "        try:\n",
    "            text = sample['prompt'] + sample['turn_1'] + sample['feedback_1'] + sample['turn_2'] + sample['feedback_2']\n",
    "        except:\n",
    "            text = sample['prompt'] + sample['turn_1'] + sample['feedback_1']\n",
    "        dataset = 'codeio'\n",
    "    else:\n",
    "        text = None\n",
    "        dataset = None\n",
    "    if add_text:\n",
    "        return {'text': f'{text}',\n",
    "                'dataset': f'{dataset}'}\n",
    "    else:\n",
    "        return {'dataset': f'{dataset}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatnating: 100%|██████████| 5/5 [00:00<00:00, 26.48it/s]\n",
      "Saving the dataset (11/11 shards): 100%|██████████| 225993/225993 [00:04<00:00, 48749.14 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['type', 'query', 'original_question', 'response', 'index', 'source', 'conversation', 'instruction', 'input', 'output', 'prompt', 'problem', 'solution', 'answer', 'problem_type', 'question_type', 'uuid', 'is_reasoning_complete', 'generations', 'correctness_math_verify', 'correctness_llama', 'finish_reasons', 'correctness_count', 'messages', 'turn_1', 'feedback_1', 'turn_2', 'feedback_2', 'dataset'],\n",
       "    num_rows: 225993\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = dataset_concat(raw_datasets)\n",
    "all_data = all_data.map(lambda sample: add_text_or_dataset(sample=sample, add_text=False))\n",
    "all_data.save_to_disk('./datasets/all_data')\n",
    "all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225993/225993 [00:07<00:00, 28820.20it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data_shuffled6 = all_data.shuffle(seed=6)\n",
    "texts = all_data_shuffled6.map(add_text_or_dataset)\n",
    "texts_extracted = texts.remove_columns([col for col in texts.column_names \\\n",
    "                                        if col != 'text' and col != 'dataset' and col != 'index'])\n",
    "\n",
    "with open('./jsonl/all_data_to_bert.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for text in tqdm(texts_extracted):\n",
    "        f.write(json.dumps(text) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_filter = TinyBERTFilter(model_path='./tinybert-filter',\n",
    "                           data_to_label='./jsonl/all_data_to_bert.jsonl')\n",
    "tb_filter.start_label()\n",
    "tb_filter.save_labelled_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['type', 'query', 'original_question', 'response', 'index', 'source', 'conversation', 'instruction', 'input', 'output', 'prompt', 'problem', 'solution', 'answer', 'problem_type', 'question_type', 'uuid', 'is_reasoning_complete', 'generations', 'correctness_math_verify', 'correctness_llama', 'finish_reasons', 'correctness_count', 'messages', 'turn_1', 'feedback_1', 'turn_2', 'feedback_2', 'dataset'],\n",
       "    num_rows: 225993\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_origin = load_from_disk('./datasets/all_data')\n",
    "filtered_data = load_from_disk('./datasets/labelled_data_filtered')\n",
    "indices = filtered_data['index']\n",
    "ds_filtered = ds_origin.select(indices)\n",
    "ds_filtered\n",
    "ds_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting: 100%|██████████| 129311/129311 [00:22<00:00, 5774.90it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 129311/129311 [00:00<00:00, 1049814.07 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['data_type', 'conversation'],\n",
       "    num_rows: 129311\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_formated = []\n",
    "for sample in tqdm(ds_filtered, desc='Formatting'):\n",
    "    if sample['query']:\n",
    "        ds_formated.append({\n",
    "            'data_type': sample['dataset'],\n",
    "            'conversation':[{\n",
    "                'from': 'human',\n",
    "                'value': sample['query']\n",
    "            }, {\n",
    "                'from': 'gpt',\n",
    "                'value': sample['response']\n",
    "            }]\n",
    "        })\n",
    "    elif sample['conversation']:\n",
    "        conversation = []\n",
    "        for dialogue in sample['conversation']:\n",
    "            conversation.append({\n",
    "                'from': 'human',\n",
    "                'value': dialogue['input']\n",
    "            })\n",
    "            conversation.append({\n",
    "                'from': 'gpt',\n",
    "                'value': dialogue['output']\n",
    "            })\n",
    "        ds_formated.append({\n",
    "            'data_type': sample['dataset'],\n",
    "            'conversation': conversation\n",
    "        })\n",
    "    elif sample['instruction']:\n",
    "        prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n",
    "        ds_formated.append({\n",
    "            'data_type': sample['dataset'],\n",
    "            'conversation': [{\n",
    "                'from': 'human',\n",
    "                'value': prompt + \"###Instruction:\\n\" + sample['instruction'] + \"###Input:\\n\" + sample['input']\n",
    "            }, {\n",
    "                'from': 'gpt',\n",
    "                'value': sample['output']\n",
    "            }]\n",
    "        })\n",
    "    elif sample['problem']:\n",
    "        ds_formated.append({\n",
    "            'data_type': sample['dataset'],\n",
    "            'conversation': [{\n",
    "                'from': 'human',\n",
    "                'value': sample['problem']\n",
    "            }, {\n",
    "                'from': 'gpt',\n",
    "                'value': sample['solution']\n",
    "            }]\n",
    "        })\n",
    "    elif sample['turn_1']:\n",
    "        if sample['turn_2']:\n",
    "            ds_formated.append({\n",
    "                'data_type': sample['dataset'],\n",
    "                'conversation': [{\n",
    "                    'from': 'human',\n",
    "                    'value': sample['prompt']\n",
    "                }, {\n",
    "                    'from': 'gpt',\n",
    "                    'value': sample['turn_1']\n",
    "                }, {\n",
    "                    'from': 'human',\n",
    "                    'value': sample['feedback_1']\n",
    "                }, {\n",
    "                    'from': 'gpt',\n",
    "                    'value': 'turn_2'\n",
    "                }]\n",
    "            })\n",
    "        else:\n",
    "            ds_formated.append({\n",
    "                'data_type': sample['dataset'],\n",
    "                'conversation': [{\n",
    "                    'from': 'human',\n",
    "                    'value': sample['prompt']\n",
    "                }, {\n",
    "                    'from': 'gpt',\n",
    "                    'value': sample['turn_1']\n",
    "                }]\n",
    "            })\n",
    "\n",
    "ds_formated = Dataset.from_list(ds_formated)\n",
    "ds_formated.save_to_disk('SFTdata')\n",
    "ds_formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metamath</th>\n",
       "      <th>openmath</th>\n",
       "      <th>capybara</th>\n",
       "      <th>code18k</th>\n",
       "      <th>codeio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <td>48133</td>\n",
       "      <td>46123</td>\n",
       "      <td>9850</td>\n",
       "      <td>14582</td>\n",
       "      <td>10623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     metamath  openmath  capybara  code18k  codeio\n",
       "num     48133     46123      9850    14582   10623"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sftdata = load_from_disk('SFTdata')\n",
    "dtype = np.array(sftdata['data_type'])\n",
    "columns=['metamath', 'openmath', 'capybara', 'code18k', 'codeio']\n",
    "counts = {col: np.sum(dtype == col).item() for col in columns}\n",
    "sftdtype = pd.DataFrame(counts, index=['num'])\n",
    "sftdtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
